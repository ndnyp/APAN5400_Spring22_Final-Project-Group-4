{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e1a1398",
   "metadata": {},
   "source": [
    "### APAN5400 Project BookStarter : ETL Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4993da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879964a4",
   "metadata": {},
   "source": [
    "#### Create function BinarySearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569ca8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BinarySearch(lys, val):\n",
    "    first = 0\n",
    "    last = len(lys)-1\n",
    "    index = -1\n",
    "    while (first <= last) and (index == -1):\n",
    "        mid = (first+last)//2\n",
    "        if lys[mid] == val:\n",
    "            index = mid\n",
    "        else:\n",
    "            if val<lys[mid]:\n",
    "                last = mid -1\n",
    "            else:\n",
    "                first = mid +1\n",
    "    if index == -1 : return False\n",
    "    else: return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2de01bc",
   "metadata": {},
   "source": [
    "Openlibrary dataset consists of 3 parts.\n",
    "\n",
    "1. Work: a collection of the similar book edition. The same book like \"Harry Potter and the Philosopher's Stone\" might have multiple edition with different language.\n",
    "2. Edition: an edition of each book which have isbn as key\n",
    "3. Authors: Information about author with author key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ce5cb3",
   "metadata": {},
   "source": [
    "New York Time provides APIs for list of all best seller history\n",
    "\n",
    "API Call Limit = 4000 request per day and 10 request per minute -> Time interval 6 seconds\n",
    "\n",
    "API Limit offset of data at 20 records per request\n",
    "\n",
    " https://developer.nytimes.com/docs/books-product/1/routes/lists/best-sellers/history.json/get"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae88e50",
   "metadata": {},
   "source": [
    "#### Download list of New York Time Bestseller books from API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47d7a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = \"ccyV1sHXrrBOAoXAJXAVAHybqbqA4QAU\"\n",
    "url = \"https://api.nytimes.com/svc/books/v3/lists/best-sellers/history.json?api-key=\" + key\n",
    "r = requests.get(url) \n",
    "nyt_history_data = r.json()\n",
    "\n",
    "all_result = nyt_history_data['num_results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7f9858",
   "metadata": {},
   "outputs": [],
   "source": [
    "numloop = all_result // 20 \n",
    "list_isbn13_nyt = []\n",
    "for i in range(0, numloop):\n",
    "    offset = i * 20\n",
    "    url = \"https://api.nytimes.com/svc/books/v3/lists/best-sellers/history.json?api-key=\" + key + \"&offset=\" + str(offset)\n",
    "    r = requests.get(url) \n",
    "    history_data = r.json()\n",
    "    try:\n",
    "        history_data['results']\n",
    "        for x in history_data['results']:\n",
    "            try:\n",
    "              isbn13 = x['isbns'][0]['isbn13']\n",
    "            except:\n",
    "              print(\"No isbn\")\n",
    "            list_isbn13_nyt.append(isbn13)\n",
    "        display(offset)\n",
    "        time.sleep(7) #Create time interval 7 seconds\n",
    "    except:\n",
    "        print(history_data)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa64c991",
   "metadata": {},
   "source": [
    "Write list of New York Time Bestseller books into text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be66a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('list_isbn13_nyt.txt', 'w') as outfile:\n",
    "    for isbn13 in list_isbn13_nyt:\n",
    "        outfile.write(isbn13 + \", \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45775d73",
   "metadata": {},
   "source": [
    "### Select random dataset from Openlibrary dataset\n",
    "\n",
    "Due to an enormous size of record from Openlibrary dataset, we randomly select record from dataset to clean, extract and load to our database (MongoDB and ElasticSearch)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06aeb1c",
   "metadata": {},
   "source": [
    "##### Select Edition which does not has ISBN13, Authors as Null and has number of page more than 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad730636",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check ISBN13 Null and Write new file only record that include isbn13,authors, and number of pages >= 50\n",
    "filename = 'ol_dump_edition_filter_isbn13_authors_notnull.txt'\n",
    "file = open(filename, \"w\")\n",
    "mil = 1\n",
    "with open('ol_dump_editions_2022-03-02.txt') as bigfile:\n",
    "    for lineno, line in enumerate(bigfile):\n",
    "        index_cut = line.find(\"{\")\n",
    "        line1 = line[index_cut:]\n",
    "        json1 = json.loads(line1)\n",
    "        try: \n",
    "            json1[\"isbn_13\"]\n",
    "            json1[\"authors\"]\n",
    "            json1[\"number_of_pages\"]\n",
    "            if(json1[\"number_of_pages\"] >= 50):\n",
    "                if lineno % 1000000 == 0: #Check Progress by print every million line\n",
    "                    print(mil)\n",
    "                    mil = mil + 1\n",
    "                file.write(line1)\n",
    "        except:\n",
    "            pass\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd5c88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read list of nyt bestseller books isbn13\n",
    "my_file = open(\"list_isbn13_nyt.txt\", \"r\")\n",
    "nyt_content = my_file.read()\n",
    "nyt_content_list = nyt_content.split(\",\")\n",
    "my_file.close()\n",
    "\n",
    "nyt_content_list1 = [x.strip(' ') for x in nyt_content_list]\n",
    "nyt_content_list1 = list(filter(None, nyt_content_list1))\n",
    "nyt_content_list = sorted(nyt_content_list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ed04c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join OL edition dump file with NYT Bestseller books by ISBN 13 and Export txt files\n",
    "filename = 'ol_dump_edition_filter_isbn13_nyt_bestseller.txt'\n",
    "file = open(filename, \"w\")\n",
    "mil = 1\n",
    "with open('ol_dump_edition_filter_isbn13_authors_notnull.txt') as bigfile:\n",
    "    for lineno, line in enumerate(bigfile):\n",
    "        json1 = json.loads(line)\n",
    "        try:\n",
    "            isbn13 = json1[\"isbn_13\"][0]\n",
    "            if BinarySearch(nyt_content_list, isbn13):\n",
    "                file.write(line)\n",
    "        except:\n",
    "            pass \n",
    "        if lineno % 1000000 == 0: #Check Progress by print every million line\n",
    "            print(mil)\n",
    "            mil = mil + 1\n",
    "        \n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bb72ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check Number of line/records of edition w/o isbn13 null\n",
    "countline = 0\n",
    "with open('ol_dump_edition_filter_isbn13_authors_notnull.txt') as bigfile:\n",
    "    for lineno, line in enumerate(bigfile):\n",
    "        countline = countline + 1\n",
    "\n",
    "print(countline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb492f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create edition record with random line + Exclude isbn13 in NYT Bestseller books list\n",
    "mil = 1\n",
    "filename = 'ol_dump_edition_random_line_edit_100000_withoutNYTBestsellers.txt'\n",
    "file = open(filename, \"w\")\n",
    "random.seed(619)\n",
    "random_edition = random.sample(range(8364970), 100000)\n",
    "random_edition = sorted(random_edition)\n",
    "with open('ol_dump_edition_filter_isbn13_authors_notnull.txt') as bigfile:\n",
    "    for lineno, line in enumerate(bigfile):\n",
    "        if BinarySearch(random_edition,lineno):\n",
    "            json1 = json.loads(line)\n",
    "            try:\n",
    "                isbn13 = json1[\"isbn_13\"][0]\n",
    "                if BinarySearch(nyt_content_list, isbn13):\n",
    "                    pass\n",
    "                else:\n",
    "                    file.write(line)\n",
    "            except:\n",
    "                pass \n",
    "        if lineno % 1000000 == 0: #Check Progress by print every million line\n",
    "            print(mil)\n",
    "            mil = mil + 1\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba554a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine Edition Random + NYT Bestseller books and Export as a file\n",
    "\n",
    "filenames = ['ol_dump_edition_random_line_edit_100000_withoutNYTBestsellers.txt','ol_dump_edition_filter_isbn13_nyt_bestseller.txt']\n",
    "\n",
    "with open('ol_dump_edition_random_line_with_nytbestseller.txt', 'w') as outfile:  \n",
    "    # Iterate through list\n",
    "    for names in filenames:\n",
    "        # Open each file in read mode\n",
    "        with open(names) as infile:\n",
    "            # read the data from file1 and\n",
    "            # file2 and write it in file3\n",
    "            outfile.write(infile.read())\n",
    "        # Add '\\n' to enter data of file2\n",
    "        # from next line\n",
    "        outfile.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b545c22",
   "metadata": {},
   "source": [
    "##### Select Author record from OpenLibrary dumpfile randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5179c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export OL Authors random\n",
    "countline = 0\n",
    "with open('ol_dump_authors_2022-03-02.txt') as bigfile:\n",
    "    for lineno, line in enumerate(bigfile):\n",
    "        countline = countline + 1\n",
    "\n",
    "print(countline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19750d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'ol_dump_authors_random_line_edit_300000.txt'\n",
    "file = open(filename, \"w\")\n",
    "random_edition = random.sample(range(9408934), 300000)\n",
    "random_edition = sorted(random_edition)\n",
    "with open('ol_dump_authors_2022-03-02.txt') as bigfile:\n",
    "    for lineno, line in enumerate(bigfile):\n",
    "        if BinarySearch(random_edition,lineno):\n",
    "            index_cut = line.find(\"{\")\n",
    "            line1 = line[index_cut:]\n",
    "            file.write(line1)\n",
    "            \n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d563201",
   "metadata": {},
   "source": [
    "##### Select Work record from OpenLibrary dumpfile randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757d0ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export OL Works random\n",
    "countline = 0\n",
    "with open('ol_dump_works_2022-03-02.txt') as bigfile:\n",
    "    for lineno, line in enumerate(bigfile):\n",
    "        countline = countline + 1\n",
    "\n",
    "print(countline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35f3070",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'ol_dump_works_random_line_edit_100000.txt'\n",
    "file = open(filename, \"w\")\n",
    "random_edition = random.sample(range(24713455), 100000)\n",
    "random_edition = sorted(random_edition)\n",
    "with open('ol_dump_works_2022-03-02.txt') as bigfile:\n",
    "    for lineno, line in enumerate(bigfile):\n",
    "        if BinarySearch(random_edition,lineno):\n",
    "            index_cut = line.find(\"{\")\n",
    "            line1 = line[index_cut:]\n",
    "            file.write(line1)\n",
    "            \n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df33af3d",
   "metadata": {},
   "source": [
    "### Load Dataset into MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3860cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install -U pymongo\n",
    "from pymongo import MongoClient\n",
    "client = MongoClient('localhost',27017) ## or MongoClient(\"localhost:27\")\n",
    "db = client.apan5400project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f006ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = open(\"ol_dump_works_random_line_edit_100000.txt\").readlines()\n",
    "jdata1 = []\n",
    "for line in json_data:\n",
    "    jdata1.append(json.loads(line))\n",
    "print(len(jdata1))\n",
    "\n",
    "json_data2 = open(\"ol_dump_edition_random_line_with_nytbestseller.txt\").readlines()\n",
    "jdata2 = []\n",
    "for line in json_data2:\n",
    "    try:\n",
    "        jdata2.append(json.loads(line))\n",
    "    except:\n",
    "        pass\n",
    "print(len(jdata2))\n",
    "\n",
    "json_data3 = open(\"ol_dump_authors_random_line_edit_300000.txt\").readlines()\n",
    "jdata3 = []\n",
    "for line in json_data3:\n",
    "    jdata3.append(json.loads(line))\n",
    "print(len(jdata3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579ea361",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection1 = db.works\n",
    "collection1.insert_many(jdata1)\n",
    "\n",
    "collection2 = db.editions\n",
    "collection2.insert_many(jdata2)\n",
    "\n",
    "collection3 = db.authors\n",
    "collection3.insert_many(jdata3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a0c66f",
   "metadata": {},
   "source": [
    "### Extract variables from dataset and load dataset into Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eca40eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "edition_result = collection2.find ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c6539e",
   "metadata": {},
   "outputs": [],
   "source": [
    "edition_list = []\n",
    "\n",
    "for record in edition_result: \n",
    "    edition_list.append(record)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b955c829",
   "metadata": {},
   "outputs": [],
   "source": [
    "jdata4 = []\n",
    "for edition in edition_list:\n",
    "    line = {}\n",
    "    try:\n",
    "        line['title'] = edition['title']\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        line['subjects'] = \", \".join(edition['subjects'])\n",
    "    except:\n",
    "        pass\n",
    "    line['authors'] = edition['authors'][0]['key']\n",
    "    line['number_of_pages'] = edition['number_of_pages']\n",
    "    line['isbn_13'] = edition['isbn_13'][0]\n",
    "    try:\n",
    "        line['revision'] = edition['revision']\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        line['latest_revision'] = edition['latest_revision']\n",
    "    except:\n",
    "        pass\n",
    "    jdata4.append(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5057e5",
   "metadata": {},
   "source": [
    "#### Extract Authors name from OpenLibrary Authors dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d765acc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_author_name = []\n",
    "mil = 1\n",
    "with open('ol_dump_authors_2022-03-02.txt') as bigfile:\n",
    "    for lineno, line in enumerate(bigfile):\n",
    "        index_cut = line.find(\"{\")\n",
    "        line1 = line[index_cut:]\n",
    "        json1 = json.loads(line1)\n",
    "        try:\n",
    "            json1['name']\n",
    "            line_app = {}\n",
    "            line_app['key'] = json1['key']\n",
    "            line_app['author'] = json1['name']\n",
    "            all_author_name.append(line_app)\n",
    "            if lineno % 1000000 == 0: #Check Progress by print every million line\n",
    "                print(mil)\n",
    "                mil = mil + 1   \n",
    "        except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9823ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_all_author_name_key = []\n",
    "for x in all_author_name:\n",
    "    list_all_author_name_key.append(x['key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8029596",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_all_author_name_author = []\n",
    "for x in all_author_name:\n",
    "    list_all_author_name_author.append(x['author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baf8361",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_list_all_author_name_key = set(list_all_author_name_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436ec80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "jdata6 = []\n",
    "mil = 1\n",
    "for edition in jdata4:\n",
    "    author = edition['authors']\n",
    "    isbn13 = edition['isbn_13']\n",
    "    line_app = {}\n",
    "    try:\n",
    "        line_app['title'] = edition['title']\n",
    "    except: pass\n",
    "    try:\n",
    "        line_app['subjects'] = edition['subjects']\n",
    "    except: pass\n",
    "    try:\n",
    "        line_app['revision'] = edition['revision']\n",
    "    except: pass\n",
    "    try:\n",
    "        line_app['latest_revision'] = edition['latest_revision']\n",
    "    except: pass\n",
    "    line_app['number_of_pages'] = edition['number_of_pages']\n",
    "    line_app['isbn_13'] = isbn13\n",
    "    line_app['NYT_bestseller'] = BinarySearch(nyt_content_list, isbn13)\n",
    "    if author in set_list_all_author_name_key:\n",
    "        index = list_all_author_name_key.index(author)\n",
    "        line_app['author'] = list_all_author_name_author[index]\n",
    "    if mil % 1000 == 0: #Check Progress by print every million line\n",
    "        print(mil)\n",
    "    mil = mil + 1    \n",
    "    jdata6.append(line_app)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76834e0",
   "metadata": {},
   "source": [
    "Write list of records for importing to ElasticSearch in text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266c304b",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_string = json.dumps(jdata6)\n",
    "es_data = open(\"edition_with_authors_name_for_es_v4.json\",\"w\")\n",
    "es_data.write(json_string)\n",
    "es_data.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047b6254",
   "metadata": {},
   "source": [
    "##### Load extract data into ElasticSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14706af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change Username and Password of ElasticSearch\n",
    "username = 'elastic'\n",
    "password = 'b3jZ+wT*4PBtnpEN8YX4'\n",
    "\n",
    "#Connect to Elasticsearch\n",
    "from elasticsearch_dsl import connections\n",
    "clientes = connections.create_connection(hosts=[\"localhost\"],\n",
    "                     port=9200, \n",
    "                     http_auth=(username, password), \n",
    "                     ca_certs='http_ca.crt',\n",
    "                     use_ssl=True, \n",
    "                     verify_certs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5abb4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Insert Edition data to Elasticsearch\n",
    "from elasticsearch.helpers import bulk\n",
    "\n",
    "resp = bulk(clientes, jdata6, index = \"edition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2baddb",
   "metadata": {},
   "source": [
    "### Load Open Library cover image into MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7a67e3",
   "metadata": {},
   "source": [
    "Load sample 1,000 ISBN13 from Edition collection from MongoDB Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2be86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# object = edition.find_one()\n",
    "result = collection2.find ({ 'isbn_13' : { '$exists': True } })\n",
    "\n",
    "# edition.find({ 'isbn_13' : { '$exists': True, '$ne': NULL } });\n",
    "\n",
    "isbn_list = []\n",
    "\n",
    "# preparing 1000 isbn for image retrieval\n",
    "for i in range(1000): \n",
    "    isbn_list.append(result[i]['isbn_13'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de2c86f",
   "metadata": {},
   "source": [
    "Create Function to download cover image from OpenLibrary APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320de6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to retrieve and download the image locally\n",
    "def download_image():\n",
    "    for i in isbn_list:\n",
    "        # prepare the isbn value and confirm size\n",
    "        isbn = i[0]\n",
    "        file_type = isbn + '-L.jpg'\n",
    "\n",
    "        # putting isbn and file_tye together to form our url\n",
    "        url = 'https://covers.openlibrary.org/b/isbn/' + file_type\n",
    "\n",
    "\n",
    "        response = requests.get(url, stream=True)\n",
    "\n",
    "        # save the image file using isbn as name\n",
    "        save_name = isbn + '.jpg'\n",
    "        with open(save_name, 'wb') as out_file:\n",
    "            shutil.copyfileobj(response.raw, out_file)\n",
    "        del response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cc688c",
   "metadata": {},
   "source": [
    "Call Download Cover Function and load the data into MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6f63b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload the image to mongodb\n",
    "import gridfs\n",
    "\n",
    "#Create a object of GridFs for the above database.\n",
    "image = gridfs.GridFS(db)\n",
    "\n",
    "#define an image object with the location.\n",
    "for i in isbn_list: \n",
    "    isbn = i[0]\n",
    "    file = isbn + '.jpg'\n",
    "\n",
    "    #Open the image in read-only format.\n",
    "    with open(file, 'rb') as f:\n",
    "        contents = f.read()\n",
    "\n",
    "    #Now store/put the image via GridFs object.\n",
    "    image.put(contents, filename=file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
